# 一：全连接层  
## 1.概念的理解  
全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。  
由于其全相连的特性，一般全连接层的参数也是最多的。  
当前面的卷积层和池化层将文件的整体分割成不同的区域后，全连接层将分散的数据重新整合成为一个新的整体  
### 本质上是一个“分类器”的作用。  
如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。  
在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积  
而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。
## 2.参数的理解  
in_features - 每个输入样本的大小  
out_features - 每个输出样本的大小  
bias - 若设置为False，这层不会学习偏置。默认值：True  
## 3.代码  
``` 
import torch  
m = torch.nn.Linear(20, 30)
input = torch.utograd.Variable(torch.randn(128, 20))
output = m(input)
print(output.size())
```  
结果：  
```  
torch.Size([128, 30])  
```  

## 1.全连接神经网络  
仅由全连接层组成的前馈神经网络。通俗来讲，是所有数据都会影响卷积出的数据，卷积出的数据是所有数据的整体，并不是局部。  
使用函数  torch.nn.Sequential.  
所有，所有神经元层必须是全连接层  
# 二：数据处理  

