# 全连接神经网络
- 定义：级联多个线性分类器来实现输入到输出的映射。
- 两层全连接网络：max函数是激活函数Relu 
- 因为线性分类器中只有一个模版记录不够准确，比如有两个马头的模版，w是权值模版，w2的行数必须跟类别个数一样，对w1没有需求。那么可以指定模版行数w1如100行，大约用4-5个模版去存储马，再用w2融合多个模版的匹配结果来实现最终类别打分。
- w1维度是100\*3072，b1维度是100\*1，f维度是100\*1
- w2维度是10\*100，b2维度是10\*1，f维度是10\*1  
## 1. 判断是否为全连接网络  
![图片](https://github.com/1511878271/Pytorch/blob/main/8.jpg)  
a是全连接神经网络，b不是  
原因：虽然对于整个网络而言，所有输入都能影响输出，但是整个网络并不是一个层，而是两个神经元层。
## 2.MLP分类  
中文：多层感知机 ，也叫人工神经网络（ANN，Artificial Neural Network）
用途：除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构  
输入层（最左边），隐藏层（中间两层），和输出层（最右边） 需要选择合适的激活函数  
![图片](https://github.com/1511878271/Pytorch/blob/main/9.png)
## 3.网络参数初始化  
### 3.1 随机初始化 
在逻辑回归的问题中，把权重参数初始化为零是可行的。但把神经网络的权重参数全部初始化为零，并使用梯度下降算法将无法获得预期的效果。  
![图片](https://github.com/1511878271/Pytorch/blob/main/10.png)  
以上面这个简单的神经网络为例。其与隐藏层关联的权重矩阵 W，是一个 2 x 2 的矩阵。现在将这个矩阵的初始值都设为 0，同样我们将偏置矩阵 b
的值也都初始化为 0。
b的初始值全为 0 不会影响最终结果，但是将权重参数矩阵W初始值都设为 0 会引起下面的问题。  
论输入什么样的特征向量 x，当进行前向传播的计算时，a1和a2大小是相等的，所以他们隐层的两个节点对输出层唯一节点的影响也是相同的。当进行反向传播的计算时，也会导致代价函数对 W1，w2偏导不会有差别。所以在更新参数后，w1和w2还是相等的。这又导致在进入下一次迭代后，依然是重蹈覆辙。
在上面这个例子中，隐藏层中上面的节点和下面的节点是相同的，它们实现的是完全相同的功能。这种情形被称为是“对称”的。当把神经网络的权重参数全部初始化为 0 后，无论运行梯度下降多长时间，所有的隐藏神经元都将依然是“对称”的。这种情况下，再多的隐层节点也是无用的，因为它们依然提供的是完全相同的功能.  
所以我们希望两个不同的隐藏单元能实现不同的功能 ，而进行**随机初始化** 能够解决这个问题。  
