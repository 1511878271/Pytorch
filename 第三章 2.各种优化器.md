# 1.AdaGrad   
可以自动变更学习速率，只需要设定一个全局的学习速率  
*注并不是实际速率，实际速率是与以往参数的模之和的开方成反比的*  
能够实现学习率的自动更改：    
梯度大，学习速率减的快一点  
梯度小，学习速率衰减的慢一点。  
 注意**深度很深的时候会造成训练提前结束**  
各种参数的说明：  
- params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
- rho (float, 可选) – 用于计算平方梯度的运行平均值的系数（默认：0.9）
- eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6）
- lr (float, 可选) – 在delta被应用到参数更新之前对它缩放的系数（默认：1.0）
- weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）
# 2.RMAProp  
通过引入一个衰减系数r，让r每回合都衰减一定的比例，类似动量的做法，相比AdaGrad，解决看过早结束的问题。   
应用方向：适合处理平稳的目标，适应卷积神经网络，引入了新的超参数，衰减系数p。  
还是依赖于***全局的学习率***。  
# 3.Adam  
本质上是带有动量的RMSprop，利用梯度的一阶矩估计和二阶估计动态调整每个参数的学习速率。  
**优点：经过偏置的矫正后，每一次迭代学习速率都拥有固定的学习范围，让参数的变化比较平稳。结合了Adagrad善于处理稀疏程度和RMSprop善于处理非平稳目标的特点，对内存的要求较小，为不同的参数计算不同的自适应学习速率，适用于大多数非凸优化，适用于大数据的收集和高维空间**
参数说明： 
- params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
- lr (float, 可选) – 学习率（默认：1e-3）
- betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）
- eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
- weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）  
***各种优化器之间的联系：***  
![图片](https://raw.githubusercontent.com/1511878271/Pytorch/main/2.jpg)
**各种优化器的学习率表达式**  
![图片](https://raw.githubusercontent.com/1511878271/Pytorch/main/3.jpg)
