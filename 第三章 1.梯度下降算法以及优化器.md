# 一.梯度下降算法  
## 1.求梯度  
尽可能最小化损失函数的一种常用的一阶算法，梯度表示了函数上升的一个趋势    
高等数学中的梯度，pytorch使用**backward（）** 求梯度  
*注需要张量中的requires_grad改为True才可以使用*  
代码示例  
```  
from math import pi  
import torch  
x = torch.tensor([pi/3,pi/6], requires_grad=True)  
f = -((x.cos()**2).sum()) ** 2  
print('value = {}'.format(f))  
f.backward()  
print('grad = {}'.format(f.grad))  
```  

## 2.梯度下降算法  
数学：对于自变量顺着梯度的方向改变一些，那么函数值就可能变大，逆着梯度改变一些，函数值就会变小一些。  
让任意函数值变尽可能变大，让函数的负值减小，从而出现梯度下降算法。  
**计算公式** ：Xt = X(t-1) - nf(X(t-1))  
使用库：torch.optim.SGD类   
代码：  
```  
import torch.optim  
optimizer = torch.optim.SGD([x, ],lr=0.1,momentum = 0)  
```   
lr 代表学习率  
## 3.梯度下降算法的局限性  
### 1.容易陷入极小值
### 2.学习率需要动态调整
为解决这些问题，pytorch拥有各种优化器来改善梯度下降算法。  
## 4.动量算法  
