# 1.过拟合的概念
在训练模型的过程中，经常会出现刚开始训练的过程中，训练和测试精度不高（或损失很大）  
当训练迭代次数的增加或不断的优化，也会出现训练精度或损失值继续改善，但精度或损失值不降反升的情况  
如图：  
![图片](https://github.com/1511878271/Pytorch/blob/main/4.jpg)  
原因：我们**优化太多了，把一些无用的数据甚至一些错误的结果都学到了，这就是过拟化**    
下面是防止过拟化的一些方法  
## 1.增大数据量  
增加数据的数量，提高准确率，就能够在实际使用过程中防止出现过拟合    
## 2.正则化  
机器学习中经常会在损失函数中加入正则项，能够提高模型在test上的准确率，能够提高模型的泛化能力所做的任何改动，我们都可以称之为正则化。   
### (1) 增大数据的特征向量  
在数据有限的情况下，在输入层用函数将数据进行放大，（如果是图像的话，在图像周围扩充0的向量对其进行放大的操作，可实现图片的扩充），提高模型在训练样本较少情况下的效果。  
### (2)dropout方法  
过拟合就是神经元过多导致无关的数据也被优化。所以dropout通过随机丢弃一部分神经元节点（特征检测器），让模型变简单，防止模型过拟合  
- 这种随机丢弃一部分神经元节点，使得两个神经元不一定都在一个网络中出现，可以减少神经元节点之间的互相作用，所谓的相互作用是指一个神经元检测的特征需要依赖其他神经元才能够发挥作用。而通过随机丢弃一部分神经元节点，可以使得神经元节点更关注自身的特征检测是否对最终的结果有帮助，而不是依赖其它有固定关系的神经元检测的特征值  
***为什么dropout会防止过拟合的发生***
- dropout机制可以看成是训练多个网络模型结构，而这些网络模型结构共享相同的参数。通过训练这样多个网络，类似bagging思想，通过多个网络的求平均结果，一定程度上可以防止过拟合，提升模型的效果。
注意：dropout只在训练阶段执行，预测的时候关闭dropout。理想情况是对所有的训练阶段的dropout网络预测一个值，然后求平均值得到预测的结果，但是这样开销太大，所以目前的做法是，在训练的时候，对没有dropout的神经元权重值做一个invert rescale，这样可以保证在预测的时候和训练的时候scale保持一致。或者，在训练的时候不做操作，在预测的时候对每个神经权重值预先乘以( 1 − d r o p o u t )，这样做的目的为了保证训练和预测权重参数scale的一致性。 
- 训练的时候，dropout类似于在input和隐层特征随机加入噪声（一定概率让一部分神经元值为0），通过加入噪声，防止模型过拟合。或者另外一个角度理解，dropout机制,会得到一个更大的样本集，一定程度上可以防止过拟合。
- dropout思想和ReLu的思想有一定的关联，dropout以一定的概率随机对神经元丢弃（输出值为0），而ReLu是对神经元小于0的值置为0。
# 2.网络定义  
### （1）nn.Module库
最常见的做法是继 承nn.Module，撰写自己的网络/层。下面先 来看看如何用nn.Module实现自己的全连接层。全连接层，又名仿射层，输出y和输入x 满足y=W x+b，W 和b是可学习的参数(我的理解是相当于一个大类，可以一环套一环  
- 自定义层Linear必须继 承nn.Module，并且在其构 造函数中需调用nn.Module 的构造函数，即 super（Linear，self）.init （）或nn.Module.init （self）
- 在构造函数 init 中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成Parameter。Parameter是一种特殊的Variable，但其默认需要求导（requires_grad=True）  
-  forward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。
- 无须写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。
- 使用时，直观上可将layer看成数学概念中的函数，调用layer（input）即可得到input对应的结果。它等价于layers.call （input），在 call 函数中，主要调用的是layer.forward（x）
- Module中的可学习参数可以通过named parameters （）或者parameters （）返回迭代器，前者会给每个parameter附上名字，使其更具有辨识度。
