# 张量的概念 
>在我的理解看来张量为创建的为一个N维数组
# 张量的创建中的小问题  
## 一：torch.tensorh和torch.Tensor的区别 
1.torch.tensor是将列表转化为张量，而torch.Tensor是直接生成指定形状或者指定数据的张量  
    
``  
In A = torch.tensor([[1.0,1.0],[2,2]])    
Out tensor([[1.,1.],[2.,2.]])
``  
   
**注意（）中有一个[]不要忘记**    
  
## 二： **requires-grad**  作用:是否计算梯度  
我理解的梯度为求导的过程  
只有计算张量，才能进行深度网络优化，根据梯度的大小进行更新  
摘抄自网络：  
如果有一个单一的输入操作需要梯度，它的输出也需要梯度。  
相反，只有所有输入都不需要梯度，输出才不需要。  
如果其中所有的变量都不需要梯度进行，后向计算不会在子图中执行。  
## 三：创建指定数据的张量函数  
torch.**_like()  **注*是指数字的英文如one，two  
``  
In torch.ones_like(2,3)  
Out [[1.,1.,1.], [1.,1.,1.]]
``    
## 四：创建随机数的张量函数  
先torch.manual_seed(123) 生成随机数的种子  
我的理解是在创建随机数时，保证每一次的随机数不同，但每次的随机数是相同的  
``  
In import torch  
   torch.manual_seed(0)  
   print(torch.rand(1))  
   print(torch.rand(1)) 
Out  tensor([0.4963])  
     tensor([0.7682])  
``  
**在保存文件后，在输入上述代码后生成的数据依然为0.4963和0.7682**  
>目前不太理解的地方是种子大小对随机数有何影响  


### 1.torch.normal(mean= **，std= ** )  
mean为指定随机数的均值，std为指定随机数的标准差  
mean 均值的概念：  
其实是针对实验观察到的特征样本而言的。比如我们实验结果得出了x1,x2,x3……xn这n个值，那么我们的均值计算是
       1/N * (x_{1}+x_{2}+…x_{n})  
**和平均值差不多，还是有一些区别**  
std标准差和高中的是一个计算方法  
**std和mean中要体现样本的形状  
### 2.torch.rand() 在[0,1]上生成均匀分布的向量（）里为张量的形状大小  
而torch.rand_like()是生成一个与其维度相同的随机数张量，（）里需为张量  
## 五：torch.arrange（），torch.linspace（）和torch.logspace（）区别  
torch.arrange（） start开始数据，end结束数据，step为步长  
torch.linspace（） start开始数据，end结束数据，step为数据的数量  
torch.logspace（）  start开始，end结束，step为数据的数量  
torch.logspace（）为以对数为间隔的向量  
  
``  
In: torch.logspace(start=0.1,end=1.0,steps=5)  
Out: tensor([1.2589, 2.1135,  3.5481,  5.9566, 10.00 ])  
``  
  
**目前不太清除logspace对数的用法**  













