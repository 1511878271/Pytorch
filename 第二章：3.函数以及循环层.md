# 一：激活函数  
代码：torch.nn.+函数名称+Cell()  
| 函数名称    | 输出范围                  | 特点 |     
| ----------- | ------                   |-----|  
| Sigmoid     |   （0,1）                 |输出远离原点时，梯度变很小|  
| ReLU        |  保留>0输出，其他会设置为0 |   只有线性关系，速度快   |    
| Tanh        | （-1,1）                  |输入数据大小对函数影响比Simoid小|
|Softplus()   |对数形式，输出无线大|对任意位置都可计算导数，保留了ReLU的特点|  
*注。权重bias会对函数图形产生影响，并影响到循环层，所有bias选择要加以考虑*
# 二：循环层  
## 1.应用  
传统神经网络输入输出都是互相独立的，而ＲＮＮ引入了一个“记忆”　　
使其实现了循环的功能，每个元素都执行相同的任务，但输出要依赖“输入”和“记忆”　　
循环神经网络的结构  
![图片](C:/Users/记忆中的你问我/Pictures/jie.png"jie")  
St = f(W*St-1 +U*Xt). W表示输入的权重, U表示此刻输入的样本的权重, V表示输出的样本权重.
Xt:表示t时刻的输入，ot:表示t时刻的输出，St:表示t时刻的记忆
这里就会用到上述激活函数中的Tanh或Relu函数，默认为Tanh  
## 2.RNN每个参数的含义  
### 1. input_size（输入编码的维度）：  
**size是维度！！！** 也就是说无论输入的东西有多少，只要每个数据都是一维的，input_size就为1  
### 2.hidden_size(隐层的特征数量):  
可理解为隐含层中，隐含节点的个数  
在文本中，此参数基本等于**序列长度**  
### 3.num_layers（RNN层数）：  
代表着每一个序列迭代的次数**不是指循环的次数** ，指输出“记忆”需要两次RNN，再以此为基础传向下一个数据  
### 4.bidirectional(bool 是否为双向RNN)  
双向是指文本中不仅要“记忆”之前的文字，同样也要关联之后的文字。  
### 5.output和h_n  
output是最后一层所有节点的hn的张量  
h_n是输出层之前的一个隐层，即将被输出的序列。

