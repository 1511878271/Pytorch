# 一：激活函数  
代码：torch.nn.+函数名称+Cell()  
| 函数名称    | 输出范围                  | 特点 |     
| ----------- | ------                   |-----|  
| Sigmoid     |   （0,1）                 |输出远离原点时，梯度变很小|  
| ReLU        |  保留>0输出，其他会设置为0 |   只有线性关系，速度快   |    
| Tanh        | （-1,1）                  |输入数据大小对函数影响比Simoid小|
|Softplus()   |对数形式，输出无线大|对任意位置都可计算导数，保留了ReLU的特点|  
*注。权重bias会对函数图形产生影响，并影响到循环层，所有bias选择要加以考虑*
# 二：循环层  
