# 一：激活函数   
代码：torch.nn.+函数名称+Cell()  
| 函数名称    | 输出范围                  | 特点                    |     
| ----------- | ------                   |-------------------------|  
| Sigmoid（S形激活）     |   （0,1）                 |输出远离原点时，梯度变很小  4.29补充在绝对值较大时导数为0，从而使梯度消失  |  
| ReLU（单侧激活）        |  保留>0输出，其他会设置为0 |   只有线性关系，速度快   |    
| Tanh（S形激活）       | （-1,1）                  |输入数据大小对函数影响比Simoid小|
|Softplus(单侧激活)   |对数形式，输出无线大|对任意位置都可计算导数，保留了ReLU的特点|  
*注。权重bias会对函数图形产生影响，并影响到循环层，所有bias选择要加以考虑*  
## 4.23补充  
### 1.激活函数的用途：用于激活层中非线性函数对张量进行逐元素运算  
激活层用于非线性运算（属于多层神经网络的部分）  
### 2.激活层的分类  
（1）用于逐元素非线性运算的逐元素激活 ，同上  
代表类型的函数（S型函数 Sigmoid）·（单侧激活 relu函数）（皱形激活 tanhshrinkkage函数）  
*三组激活函数各有利弊，在使用时需要逐类使用观察效果，再分别使用具体类型的函数*
（2）多元素组合运算的激活层  
大多与softmax（）函数有关
#### 3.softmax类型函数  
（1）softmax（）对张量的某一维度进行   
（2）softmax2d（）对四维张量做运算
# 二：循环层  
## 1.应用  
传统神经网络输入输出都是互相独立的，而ＲＮＮ引入了一个“记忆”　　
使其实现了循环的功能，每个元素都执行相同的任务，但输出要依赖“输入”和“记忆”　　
循环神经网络的结构  
![图片](C:/Users/记忆中的你问我/Pictures/jie.png"jie")  
St = f(W*St-1 +U*Xt). W表示输入的权重, U表示此刻输入的样本的权重, V表示输出的样本权重.
Xt:表示t时刻的输入，ot:表示t时刻的输出，St:表示t时刻的记忆
这里就会用到上述激活函数中的Tanh或Relu函数，默认为Tanh  
## 2.RNN每个参数的含义  
### 1. input_size（输入编码的维度）：  
**size是维度！！！** 也就是说无论输入的东西有多少，只要每个数据都是一维的，input_size就为1  
### 2.hidden_size(隐层的特征数量):  
可理解为隐含层中，隐含节点的个数  
在文本中，此参数基本等于**序列长度**  
### 3.num_layers（RNN层数）：  
代表着每一个序列迭代的次数**不是指循环的次数** ，指输出“记忆”需要两次RNN，再以此为基础传向下一个数据  
### 4.bidirectional(bool 是否为双向RNN)  
双向是指文本中不仅要“记忆”之前的文字，同样也要关联之后的文字。  
### 5.output和h_n  
output是最后一层所有节点的hn的张量  
h_n是输出层之前的一个隐层，即将被输出的序列。

